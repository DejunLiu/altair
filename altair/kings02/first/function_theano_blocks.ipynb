{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "#os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu,optimizer=fast_compile'\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin/\"\n",
    "#import sys\n",
    "#sys.path.append('/data/fs4/home/bradh/')\n",
    "import theano\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import learningfunctions\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, LSTM\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/local_data/kylez/altair_runs/fn0_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ['this is data', 'we like data', 'once upon a time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from redbaron import RedBaron\n",
    "\n",
    "with open(\"sample.py\", \"r\") as f:\n",
    "    source = f.read()\n",
    "with open(\"sample.py\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "red = RedBaron(source)\n",
    "data = []\n",
    "for fn_node in red.findAll(\"DefNode\"):\n",
    "    starting_line = fn_node.absolute_bounding_box.top_left.to_tuple()[0]\n",
    "    ending_line = fn_node.absolute_bounding_box.bottom_right.to_tuple()[0]\n",
    "    fn_lines = lines[starting_line-1:ending_line-1]\n",
    "    data.append(\"\".join(fn_lines).rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = data[:3]\n",
    "data = [d[:20] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNK token will be second to last dimension\n",
    "# EOS token will always be the last dimension\n",
    "# If desired_length is not specified, desired_length will be len(document)\n",
    "# If len(document) < desired_length, add an EOS token an pad with zero vectors to reach desired_length\n",
    "# If len(document) > desired_length, truncate to desired_length\n",
    "def encode_document(document, desired_length=-1, min_unicode_idx=0, max_unicode_idx=128):\n",
    "    UNK_IDX = max_unicode_idx\n",
    "    EOS_IDX = max_unicode_idx + 1\n",
    "    if desired_length == -1:\n",
    "        desired_length = len(document)\n",
    "    encoded = np.zeros((desired_length, max_unicode_idx-min_unicode_idx+2)) # +2 for UNK and EOS tokens\n",
    "    for doc_idx, char in enumerate(document[:desired_length]):\n",
    "        char_encoding = ord(char)\n",
    "        if not min_unicode_idx <= char_encoding < max_unicode_idx:\n",
    "            char_encoding = UNK_IDX\n",
    "        encoded[doc_idx, char_encoding-min_unicode_idx] = 1\n",
    "    if len(document) < desired_length:\n",
    "        encoded[len(document[:desired_length]), EOS_IDX-min_unicode_idx] = 1\n",
    "    #encoded[len(document[:desired_length]), EOS_IDX-min_unicode_idx] = 1\n",
    "    return encoded.reshape(encoded.shape[0], 1, encoded.shape[1])\n",
    "\n",
    "# By default, desired_length will be the length of the longest document in documents.\n",
    "def encode_documents(documents, desired_length=-1, min_unicode_idx=0, max_unicode_idx=128):\n",
    "    if desired_length == -1:\n",
    "        desired_length = max([len(document) for document in documents])\n",
    "    encodeds = []\n",
    "    for document in documents:\n",
    "        encodeds.append(encode_document(document, desired_length, min_unicode_idx, max_unicode_idx))\n",
    "    e = np.array(encodeds)\n",
    "    return e\n",
    "\n",
    "# encoded must be one-hot, encoded via encode_document()\n",
    "def decode_document(encoded, min_unicode_idx=0, max_unicode_idx=128, unk_decode_idx=32):\n",
    "    UNK_IDX = max_unicode_idx\n",
    "    EOS_IDX = max_unicode_idx + 1\n",
    "    decoded = \"\"\n",
    "    for idx in np.nonzero(encoded)[2]:\n",
    "        candidate = idx + min_unicode_idx\n",
    "        if candidate == UNK_IDX:\n",
    "            candidate = unk_decode_idx\n",
    "        elif candidate == EOS_IDX:\n",
    "            break\n",
    "        decoded += chr(candidate)\n",
    "    return decoded\n",
    "\n",
    "def decode_documents(encodeds, min_unicode_idx=0, max_unicode_idx=128):\n",
    "    decodeds = []\n",
    "    for encoded in encodeds:\n",
    "        decodeds.append(decode_document(encoded, min_unicode_idx, max_unicode_idx))\n",
    "    return decodeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_conversion(predictions):\n",
    "    converted = np.zeros(predictions.shape)\n",
    "    for prediction_idx, prediction in enumerate(predictions):\n",
    "        for elem_idx, elem in enumerate(prediction):\n",
    "            converted[prediction_idx, elem_idx, 0, np.argmax(elem[0])] = 1\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('X')\n",
    "rnnType = 'gru'\n",
    "dimIn = 130\n",
    "dim = 20\n",
    "wtstd = 0.5\n",
    "rnnbias_init = Constant(0.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "\n",
    "lr = 0.0001\n",
    "decay = 0.9\n",
    "decay_itr = 15000\n",
    "pickle_itr = 10000\n",
    "learning_rate = theano.shared(np.array(lr, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(decay, dtype=theano.config.floatX)\n",
    "\n",
    "clippings = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_step(data_in, fork, rnn):\n",
    "    def step(data_in):\n",
    "        d1, d2 = fork.apply(data_in)\n",
    "        result = rnn.apply(d1, d2)\n",
    "        return result\n",
    "    hEnc, _ = theano.scan(step, data_in)\n",
    "    return hEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding Step\n",
    "dimMultiplier = 2\n",
    "\n",
    "relu = Rectifier()\n",
    "rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru', activation=relu)\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc = encoding_step(X, fork, rnn)\n",
    "\n",
    "rnn2 = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru', activation=relu)\n",
    "fork2 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc2 = encoding_step(hEnc, fork2, rnn2)\n",
    "\n",
    "rnn3 = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru', activation=relu)\n",
    "fork3 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc3 = encoding_step(hEnc2, fork3, rnn3)\n",
    "\n",
    "forkD = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dimIn, dimIn * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "h4decoder = hEnc3[:,-1,:,:].reshape((-1, 1,1,20))\n",
    "h4reshape, _ = forkD.apply(h4decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decoding Step\n",
    "rnn4 = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "#TOTHINK: transform before the decoder or after\n",
    "fork4 = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dimIn, dimIn * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "targets = T.concatenate((h4reshape, X[:,:-1, :,:]), axis=1)\n",
    "hDec = encoding_step(targets, fork4, rnn4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forks = [fork, fork2, fork3, fork4, forkD]\n",
    "rnns = [rnn, rnn2, rnn3, rnn4]\n",
    "\n",
    "for fork in forks:\n",
    "    fork.initialize()\n",
    "for rnn in rnns:\n",
    "    rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predTargets = T.exp(hDec)/T.sum(T.exp(hDec), axis=(3,2), keepdims=True)\n",
    "#precost = -X.squeeze*T.log(predTargets.squeeze()) - (1-X.squeeze())*T.log(1-predTargets.squeeze())\n",
    "\n",
    "#ADDLATER: beam search\n",
    "cost = T.mean(T.sum(T.nnet.categorical_crossentropy(predTargets, X), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cg = ComputationGraph([cost])\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "\n",
    "###To check gradients for explosion/shrinkage\n",
    "print('compiling graph you talented soul')\n",
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, clippings)\n",
    "gradientFun = theano.function([X, predTargets], gradients, allow_input_downcast=True)\n",
    "print('finished gradientFun')\n",
    "\n",
    "learning = learningfunctions.Learning(cost,params,learning_rate,l1=0.,l2=0.,maxnorm=0.,c=clippings)\n",
    "updates = learning.Adam() \n",
    "\n",
    "classifierTrain = theano.function([X], [cost, predTargets], \n",
    "                                  updates=updates, allow_input_downcast=True)\n",
    "print('finished classifierTrain')\n",
    "#classifierPredict = theano.function([X], [softoutClass, attEncpred, attContextpred], allow_input_downcast=True)\n",
    "classifierPredict = theano.function([X], predTargets, allow_input_downcast=True)\n",
    "print('finished classifierPredict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifierTrain.get_shared()[0].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100000\n",
    "cur_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_unicode = 0\n",
    "max_unicode = 128\n",
    "encoded = encode_documents(data, min_unicode_idx=min_unicode, max_unicode_idx=max_unicode)\n",
    "for epoch in range(cur_epoch, num_epochs):\n",
    "    result_cost, result_predTargets = classifierTrain(encoded)\n",
    "    result_converted = one_hot_conversion(result_predTargets)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"%s: %s\" % (result_cost, decode_documents(result_converted, min_unicode_idx=min_unicode, max_unicode_idx=max_unicode)))\n",
    "        grads = gradientFun(encoded, result_predTargets)\n",
    "        for gra in grads:\n",
    "            print('  gradient norms: ', np.linalg.norm(gra))\n",
    "    if epoch % decay_itr == 0:\n",
    "        learning_rate.set_value(learning_rate.get_value() * learning_decay)\n",
    "    if epoch % pickle_itr == 0:\n",
    "        with open(os.path.join(MODEL_DIR, \"classifierTrain.%s.mdl\" % epoch), \"wb\") as f:\n",
    "            pickle.dump(classifierTrain, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(MODEL_DIR, \"gradientFun.%s.mdl\" % epoch), \"wb\") as f:\n",
    "            pickle.dump(gradientFun, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(MODEL_DIR, \"costs.txt\"), \"a\") as f:\n",
    "            f.write(\"%s\\n\" % result_cost)\n",
    "    cur_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifierTrain.get_shared()[0].get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Altair",
   "language": "python",
   "name": "altair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: TITAN X (Pascal) (CNMeM is disabled, cuDNN 5105)\n",
      "/opt/conda/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin/\"\n",
    "import sys\n",
    "sys.path.append('/data/fs4/home/bradh/')\n",
    "import theano\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import learningfunctions\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, LSTM\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_step(data_in, fork, rnn):\n",
    "    def step(data_in):\n",
    "        d1, d2 = fork.apply(data_in)\n",
    "        result = rnn.apply(d1, d2)\n",
    "        return result\n",
    "    hEnc, _ = theano.scan(step, data_in)\n",
    "    return hEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xf = T.tensor4('Xf')\n",
    "Xi = T.matrix('Xi')\n",
    "Xs = T.tensor4('Xs')\n",
    "\n",
    "dimInf = 50\n",
    "dimIni = 60\n",
    "dimIns = 30\n",
    "dimComp = 10\n",
    "\n",
    "wtstd = 0.1\n",
    "rnnbias_init = Constant(0.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "classifierWts = IsotropicGaussian(wtstd)\n",
    "classifierBias = Constant(0.0001)\n",
    "\n",
    "clippings = 1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "fun = 0\n",
    "actFunctList = [Rectifier(), Tanh()] #LeakyRectifier(leak=0.01)\n",
    "actFunct = actFunctList[fun]\n",
    "\n",
    "dimMultiplier = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FUNCTION ENCODER\n",
    "rnnf = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'gru', activation=actFunct)\n",
    "forkf = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimInf, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEncf = encoding_step(Xf, forkf, rnnf)\n",
    "\n",
    "rnn2f = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork2f = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc2f = encoding_step(hEncf, fork2f, rnn2f)\n",
    "\n",
    "rnn3f = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork3f = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc3f = encoding_step(hEnc2f, fork3f, rnn3f)\n",
    "\n",
    "#forkDf = Fork(output_names=['linear', 'gates'],\n",
    "#            name='fork', input_dim=dimComp, output_dims=[dimInf, dimInf * dimMultiplier], \n",
    "#            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "h4decoderf = hEnc3f[:,-1,:,:].reshape((-1, 1,1,dimComp))\n",
    "#h4reshapef, _ = forkDf.apply(h4decoderf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMPORT ENCODER\n",
    "encoder_bmlp = BatchNormalizedMLP( activations=[actFunct], \n",
    "           dims=[dimIni, dimComp],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init= classifierBias)\n",
    "\n",
    "encoder_output = encoder_bmlp.apply(Xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SEMANTIC ENCODER\n",
    "rnns = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'gru', activation=actFunct)\n",
    "forks = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIns, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEncs = encoding_step(Xs, forks, rnns)\n",
    "\n",
    "rnn2s = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork2s = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc2s = encoding_step(hEncs, fork2s, rnn2s)\n",
    "\n",
    "rnn3s = GatedRecurrent(dim=dimComp, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork3s = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp, output_dims=[dimComp, dimComp * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hEnc3s = encoding_step(hEnc2s, fork3s, rnn3s)\n",
    "\n",
    "#forkDs = Fork(output_names=['linear', 'gates'],\n",
    "#            name='fork', input_dim=dimComp, output_dims=[dimIns, dimIns * dimMultiplier], \n",
    "#            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "h4decoders = hEnc3s[:,-1,:,:].reshape((-1, 1,1,dimComp))\n",
    "#h4reshapes, _ = forkDs.apply(h4decoders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: add beam search\n",
    "#TOTHINK: transform before the decoder or after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT VECTOR SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CONCATENATE\n",
    "jvs = T.concatenate([T.squeeze(h4decoderf), encoder_output, T.squeeze(h4decoders)], axis = 1)\n",
    "jvsReshaped = T.reshape(jvs, (-1, 1,1,dimComp*3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FUNCTION DECODER\n",
    "rnn4f = GatedRecurrent(dim=dimInf, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork4f = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp*3, output_dims=[dimInf, dimInf * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hDecf = encoding_step(jvsReshaped, fork4f, rnn4f)\n",
    "targetsf = T.concatenate((hDecf, Xf[:,:-1, :,:]), axis=1)\n",
    "\n",
    "\n",
    "predictedXf = T.exp(targetsf)/T.sum(T.exp(targetsf), axis=(3,2), keepdims=True)\n",
    "costf = T.mean(T.sum(T.nnet.categorical_crossentropy(predictedXf, Xf), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test = theano.function([Xf], hEncf, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fakeXf = np.random.rand(13, 30, 1, dimInf)\n",
    "fakeXi = np.random.rand(13, dimIni)\n",
    "fakeXs = np.random.rand(13, 100, 1, dimIns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test(fakeXf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IMPORT DECODER\n",
    "decoder_bmlp = BatchNormalizedMLP( activations=[actFunct], \n",
    "           dims=[dimComp*3, dimIni],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init=classifierBias )\n",
    "\n",
    "decoder_output = decoder_bmlp.apply(jvs)\n",
    "\n",
    "predictedXi = T.exp(decoder_output)/T.sum(T.exp(decoder_output), axis = 1, keepdims=True)\n",
    "costi = T.mean(CategoricalCrossEntropy().apply(Xi, predictedXi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SEMANTIC DECODER\n",
    "rnn4s = GatedRecurrent(dim=dimIns, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                       name = 'gru', activation=actFunct)\n",
    "fork4s = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimComp*3, output_dims=[dimIns, dimIns * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "hDecs = encoding_step(jvsReshaped, fork4s, rnn4s)\n",
    "targetss = T.concatenate((hDecs, Xs[:,:-1, :,:]), axis=1)\n",
    "\n",
    "\n",
    "predictedXs = T.exp(targetss)/T.sum(T.exp(targetss), axis=(3,2), keepdims=True)\n",
    "costs = T.mean(T.sum(T.nnet.categorical_crossentropy(predictedXs, Xs), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#INTIALIZE\n",
    "fork = [forkf, fork2f, fork3f, fork4f, forks, fork2s, fork3s, fork4s]\n",
    "rnn = [rnnf, rnn2f, rnn3f, rnn4f, rnns, rnn2s, rnn3s, rnn4s]\n",
    "mlp = [encoder_bmlp, decoder_bmlp]\n",
    "\n",
    "for finit in fork:\n",
    "    finit.initialize()\n",
    "for rinit in rnn:\n",
    "    rinit.initialize()\n",
    "for minit in mlp:\n",
    "    minit.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling graph you talented soul\n",
      "finished compiling\n"
     ]
    }
   ],
   "source": [
    "cost = costf+costi+costs\n",
    "cg = ComputationGraph([cost])\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "\n",
    "learning = learningfunctions.Learning(cost,params,learning_rate,l1=0.,l2=0.,maxnorm=0.,c=clippings)\n",
    "updates = learning.Adam() \n",
    "\n",
    "print('compiling graph you talented soul')\n",
    "train = theano.function([Xf,Xi,Xs], cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function([Xf,Xi,Xs], [predictedXf, predictedXi, predictedXs], allow_input_downcast=True)\n",
    "print('finished compiling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.03048947,  0.01547793,  0.0117109 , ...,  0.02137607,\n",
       "            0.01571949,  0.03888531]],\n",
       " \n",
       "         [[ 0.01336728,  0.0158511 ,  0.01565423, ...,  0.01373032,\n",
       "            0.01990004,  0.02536408]],\n",
       " \n",
       "         [[ 0.01279809,  0.01629212,  0.01187036, ...,  0.01796917,\n",
       "            0.0194895 ,  0.01943433]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.0123929 ,  0.02321035,  0.02288632, ...,  0.01349519,\n",
       "            0.01841797,  0.01474056]],\n",
       " \n",
       "         [[ 0.01795548,  0.02480968,  0.01189146, ...,  0.01939003,\n",
       "            0.02043458,  0.02170853]],\n",
       " \n",
       "         [[ 0.03086285,  0.02438878,  0.01831656, ...,  0.03069382,\n",
       "            0.0126109 ,  0.02171484]]],\n",
       " \n",
       " \n",
       "        [[[ 0.0307118 ,  0.01511004,  0.01143822, ...,  0.02172776,\n",
       "            0.01649292,  0.04105354]],\n",
       " \n",
       "         [[ 0.01584915,  0.02096317,  0.02028376, ...,  0.01433983,\n",
       "            0.01677924,  0.01198772]],\n",
       " \n",
       "         [[ 0.01236266,  0.01304613,  0.01977151, ...,  0.02040976,\n",
       "            0.03123224,  0.01624589]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.015516  ,  0.01171494,  0.02238931, ...,  0.01528896,\n",
       "            0.02547715,  0.01651972]],\n",
       " \n",
       "         [[ 0.02829328,  0.02355534,  0.01495865, ...,  0.01251923,\n",
       "            0.01441621,  0.01336382]],\n",
       " \n",
       "         [[ 0.02203023,  0.02041395,  0.02344464, ...,  0.02403348,\n",
       "            0.02838983,  0.01530206]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03048862,  0.01583491,  0.01174896, ...,  0.02091501,\n",
       "            0.01606108,  0.03864261]],\n",
       " \n",
       "         [[ 0.0151446 ,  0.02249981,  0.01399894, ...,  0.02147354,\n",
       "            0.0251477 ,  0.02942699]],\n",
       " \n",
       "         [[ 0.01844264,  0.01918397,  0.02249899, ...,  0.01249886,\n",
       "            0.01344018,  0.01461851]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.01261029,  0.02219027,  0.0319888 , ...,  0.01683352,\n",
       "            0.0227287 ,  0.03074158]],\n",
       " \n",
       "         [[ 0.01291272,  0.01305248,  0.02439535, ...,  0.03144648,\n",
       "            0.02231221,  0.01494791]],\n",
       " \n",
       "         [[ 0.0186119 ,  0.01470378,  0.03041354, ...,  0.0151689 ,\n",
       "            0.01284308,  0.02636506]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ 0.03330279,  0.01611552,  0.01147851, ...,  0.02063119,\n",
       "            0.01539331,  0.04072292]],\n",
       " \n",
       "         [[ 0.02435537,  0.01620804,  0.01661271, ...,  0.01548842,\n",
       "            0.02913782,  0.01529141]],\n",
       " \n",
       "         [[ 0.01165473,  0.02074686,  0.01471479, ...,  0.02421554,\n",
       "            0.02643587,  0.01945007]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.01388948,  0.0203087 ,  0.02102928, ...,  0.01606381,\n",
       "            0.01944864,  0.02150053]],\n",
       " \n",
       "         [[ 0.01432236,  0.02154432,  0.01452565, ...,  0.02187513,\n",
       "            0.02033219,  0.0277628 ]],\n",
       " \n",
       "         [[ 0.01499651,  0.01392115,  0.02246241, ...,  0.02551406,\n",
       "            0.01869052,  0.02930076]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03137724,  0.01536259,  0.01136787, ...,  0.02115361,\n",
       "            0.01622937,  0.04117735]],\n",
       " \n",
       "         [[ 0.02365845,  0.03350834,  0.01326669, ...,  0.03063398,\n",
       "            0.01322751,  0.02297437]],\n",
       " \n",
       "         [[ 0.02721047,  0.01541296,  0.01539406, ...,  0.01243934,\n",
       "            0.01710486,  0.01825362]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.01606081,  0.01168378,  0.01173456, ...,  0.02062909,\n",
       "            0.0144987 ,  0.01555307]],\n",
       " \n",
       "         [[ 0.01530559,  0.01896392,  0.01775273, ...,  0.02021125,\n",
       "            0.01091818,  0.0280839 ]],\n",
       " \n",
       "         [[ 0.01608209,  0.01215741,  0.0143593 , ...,  0.0268097 ,\n",
       "            0.01272081,  0.01746581]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03181342,  0.01495541,  0.01165147, ...,  0.02280916,\n",
       "            0.01602903,  0.04241457]],\n",
       " \n",
       "         [[ 0.02250048,  0.02378655,  0.0198829 , ...,  0.01538813,\n",
       "            0.01612594,  0.02649171]],\n",
       " \n",
       "         [[ 0.01544065,  0.03003453,  0.01265392, ...,  0.01788602,\n",
       "            0.02227712,  0.01279575]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.0160501 ,  0.01355208,  0.03077948, ...,  0.01720189,\n",
       "            0.02720109,  0.01880757]],\n",
       " \n",
       "         [[ 0.01376803,  0.02027862,  0.01579501, ...,  0.0203211 ,\n",
       "            0.02067759,  0.02493636]],\n",
       " \n",
       "         [[ 0.0175334 ,  0.01734488,  0.0199748 , ...,  0.01992096,\n",
       "            0.02250489,  0.01438604]]]], dtype=float32),\n",
       " array([[ 0.02372171,  0.0141833 ,  0.01367402,  0.01332242,  0.01641458,\n",
       "          0.01332242,  0.01332242,  0.01332242,  0.02639661,  0.01332242,\n",
       "          0.01332242,  0.01335792,  0.01332242,  0.01332242,  0.01332242,\n",
       "          0.01332242,  0.01332242,  0.01332242,  0.01332242,  0.02279176,\n",
       "          0.01332242,  0.01735198,  0.01459845,  0.01416092,  0.01936847,\n",
       "          0.01332242,  0.02042508,  0.01378062,  0.01332242,  0.01332242,\n",
       "          0.01332242,  0.01332242,  0.01332242,  0.01332242,  0.01332242,\n",
       "          0.01332242,  0.01981122,  0.01332242,  0.01834186,  0.01332242,\n",
       "          0.01708854,  0.01332242,  0.01681886,  0.02129034,  0.02993693,\n",
       "          0.01509683,  0.0148923 ,  0.01332242,  0.01332242,  0.0205969 ,\n",
       "          0.02350536,  0.01839193,  0.01358366,  0.01446302,  0.07274093,\n",
       "          0.02319703,  0.01332242,  0.01332242,  0.01702373,  0.01332242],\n",
       "        [ 0.02433565,  0.0140825 ,  0.01364711,  0.01330871,  0.0160452 ,\n",
       "          0.01330871,  0.01330871,  0.01330871,  0.02805129,  0.01330871,\n",
       "          0.01330871,  0.01330871,  0.01330871,  0.01330871,  0.01330871,\n",
       "          0.01330871,  0.01330871,  0.01330871,  0.01330871,  0.02130699,\n",
       "          0.01330871,  0.01774524,  0.01428754,  0.01418414,  0.0189828 ,\n",
       "          0.01330871,  0.01931814,  0.01400491,  0.01330871,  0.01330871,\n",
       "          0.01330871,  0.01330871,  0.01330871,  0.01330871,  0.01330871,\n",
       "          0.01330871,  0.01804822,  0.01333782,  0.01758095,  0.01330871,\n",
       "          0.01879243,  0.01330871,  0.0173001 ,  0.02097656,  0.02826108,\n",
       "          0.01426042,  0.01471771,  0.01330871,  0.01330871,  0.02065001,\n",
       "          0.02435416,  0.02110379,  0.01330871,  0.01563982,  0.07608879,\n",
       "          0.01986961,  0.01330871,  0.01330871,  0.01714815,  0.01330871],\n",
       "        [ 0.02425984,  0.01454968,  0.01399121,  0.013322  ,  0.01572156,\n",
       "          0.013322  ,  0.013322  ,  0.013322  ,  0.02717311,  0.013322  ,\n",
       "          0.013322  ,  0.013322  ,  0.013322  ,  0.013322  ,  0.013322  ,\n",
       "          0.013322  ,  0.013322  ,  0.013322  ,  0.013322  ,  0.02167767,\n",
       "          0.013322  ,  0.0174999 ,  0.01394822,  0.01346073,  0.02009275,\n",
       "          0.013322  ,  0.02013073,  0.01374753,  0.013322  ,  0.013322  ,\n",
       "          0.013322  ,  0.013322  ,  0.013322  ,  0.013322  ,  0.013322  ,\n",
       "          0.013322  ,  0.01954472,  0.01344244,  0.01727365,  0.013322  ,\n",
       "          0.01835256,  0.013322  ,  0.01726489,  0.0203074 ,  0.0285045 ,\n",
       "          0.01430809,  0.01474368,  0.013322  ,  0.013322  ,  0.02075826,\n",
       "          0.02375819,  0.01968453,  0.01425752,  0.01487808,  0.07609222,\n",
       "          0.02031128,  0.013322  ,  0.013322  ,  0.01728303,  0.013322  ],\n",
       "        [ 0.02435314,  0.01419287,  0.01380324,  0.01333221,  0.01608255,\n",
       "          0.01333221,  0.01333221,  0.01333221,  0.02735589,  0.01333221,\n",
       "          0.01333221,  0.01333221,  0.01333221,  0.01333221,  0.01333221,\n",
       "          0.01333221,  0.01333221,  0.01333221,  0.01333221,  0.02035834,\n",
       "          0.01333221,  0.0180204 ,  0.01456769,  0.01366466,  0.01933995,\n",
       "          0.01333221,  0.01892515,  0.01425207,  0.01333221,  0.01333221,\n",
       "          0.01333221,  0.01333221,  0.01333221,  0.01333221,  0.01333221,\n",
       "          0.01333221,  0.01974116,  0.01333221,  0.01739898,  0.01333221,\n",
       "          0.01905927,  0.01333221,  0.01750091,  0.02026742,  0.02825685,\n",
       "          0.01410258,  0.01476945,  0.01333221,  0.01333221,  0.02056477,\n",
       "          0.02295879,  0.02095297,  0.01377005,  0.0158087 ,  0.07614184,\n",
       "          0.01989069,  0.01333221,  0.01333221,  0.01726894,  0.01333221],\n",
       "        [ 0.0246657 ,  0.01541189,  0.01423246,  0.01334773,  0.015165  ,\n",
       "          0.01334773,  0.01334773,  0.01334773,  0.02778019,  0.01334773,\n",
       "          0.01337613,  0.01334773,  0.01334773,  0.01334773,  0.01334773,\n",
       "          0.01334773,  0.01334773,  0.01334773,  0.01334773,  0.02295342,\n",
       "          0.01351572,  0.01625418,  0.01334773,  0.01342887,  0.02289641,\n",
       "          0.01334773,  0.02085825,  0.01334773,  0.01334773,  0.01334773,\n",
       "          0.01334773,  0.01334773,  0.01334773,  0.01334773,  0.01334773,\n",
       "          0.01334773,  0.01815003,  0.01401746,  0.01713824,  0.01334773,\n",
       "          0.01908934,  0.01334773,  0.01680239,  0.02082089,  0.02736411,\n",
       "          0.01461585,  0.01471962,  0.01334773,  0.01334773,  0.02027456,\n",
       "          0.02473987,  0.0198046 ,  0.01423801,  0.01334773,  0.07291823,\n",
       "          0.02078675,  0.01334773,  0.01334773,  0.01685439,  0.01334773],\n",
       "        [ 0.02456938,  0.01490354,  0.01425069,  0.01330174,  0.01503443,\n",
       "          0.01330174,  0.01330174,  0.01330174,  0.0265245 ,  0.01330174,\n",
       "          0.0133613 ,  0.01330174,  0.01330174,  0.01330174,  0.01330174,\n",
       "          0.01330174,  0.01330174,  0.01330174,  0.01330174,  0.02228463,\n",
       "          0.01330174,  0.01695778,  0.01341988,  0.01378485,  0.0209152 ,\n",
       "          0.01330174,  0.02088303,  0.01376274,  0.01330174,  0.01330174,\n",
       "          0.01330174,  0.01330174,  0.01330174,  0.01330174,  0.01330174,\n",
       "          0.01330174,  0.01898742,  0.01361695,  0.01734222,  0.01330174,\n",
       "          0.01863157,  0.01330174,  0.01745347,  0.02122593,  0.02829829,\n",
       "          0.01461629,  0.01461141,  0.01330174,  0.01330174,  0.02029718,\n",
       "          0.02411174,  0.02042221,  0.01447155,  0.01432779,  0.07399491,\n",
       "          0.02064731,  0.01330174,  0.01330174,  0.01723961,  0.01330174],\n",
       "        [ 0.02443382,  0.01418645,  0.01379277,  0.01333394,  0.0156914 ,\n",
       "          0.01333394,  0.01333394,  0.01333394,  0.02768603,  0.01333394,\n",
       "          0.01360263,  0.01333394,  0.01333394,  0.01333394,  0.01333394,\n",
       "          0.01333394,  0.01333394,  0.01333394,  0.01333394,  0.02105241,\n",
       "          0.01333394,  0.01741016,  0.01380868,  0.01447406,  0.01962802,\n",
       "          0.01333394,  0.01931307,  0.01411286,  0.01333394,  0.01333394,\n",
       "          0.01333394,  0.01333394,  0.01333394,  0.01333394,  0.01333394,\n",
       "          0.01333394,  0.01796062,  0.0136873 ,  0.01742819,  0.01333394,\n",
       "          0.01932125,  0.01333394,  0.01745214,  0.02081176,  0.02806924,\n",
       "          0.01455978,  0.01450848,  0.01333394,  0.01333394,  0.02046151,\n",
       "          0.02355209,  0.02241838,  0.01355891,  0.01516838,  0.07421645,\n",
       "          0.02012115,  0.01333394,  0.01333394,  0.01749364,  0.01333394],\n",
       "        [ 0.02515455,  0.01435065,  0.01395509,  0.01327789,  0.01653814,\n",
       "          0.01327789,  0.01327789,  0.01327789,  0.02895629,  0.01327789,\n",
       "          0.01327789,  0.01327789,  0.01327789,  0.01327789,  0.01327789,\n",
       "          0.01327789,  0.01327789,  0.01327789,  0.01327789,  0.02103181,\n",
       "          0.01327789,  0.01816373,  0.01481972,  0.01327789,  0.01907496,\n",
       "          0.01327789,  0.01893727,  0.0135864 ,  0.01327789,  0.01327789,\n",
       "          0.01327789,  0.01327789,  0.01327789,  0.01327789,  0.01327789,\n",
       "          0.01327789,  0.01907528,  0.01327789,  0.01712026,  0.01327789,\n",
       "          0.01853867,  0.01327789,  0.01713408,  0.02022388,  0.0275988 ,\n",
       "          0.01340314,  0.01504085,  0.01327789,  0.01327789,  0.0211981 ,\n",
       "          0.02375693,  0.01930768,  0.01329515,  0.01623786,  0.08057024,\n",
       "          0.01833904,  0.01327789,  0.01327789,  0.01642084,  0.01327789],\n",
       "        [ 0.02403166,  0.01443492,  0.01366551,  0.01332273,  0.01559666,\n",
       "          0.01332273,  0.01332273,  0.01332273,  0.02710829,  0.01332273,\n",
       "          0.01368598,  0.01332273,  0.01332273,  0.01332273,  0.01332273,\n",
       "          0.01332273,  0.01332273,  0.01332273,  0.01332273,  0.0216325 ,\n",
       "          0.01332273,  0.01710317,  0.01381108,  0.0143284 ,  0.01943908,\n",
       "          0.01332273,  0.02007981,  0.01395219,  0.01332273,  0.01332273,\n",
       "          0.01332273,  0.01332273,  0.01332273,  0.01332273,  0.01332273,\n",
       "          0.01332273,  0.01855608,  0.01375839,  0.01753087,  0.01332273,\n",
       "          0.01814337,  0.01332273,  0.01713612,  0.02118435,  0.02893674,\n",
       "          0.01494284,  0.01460809,  0.01332273,  0.01332273,  0.02060795,\n",
       "          0.02426458,  0.02100299,  0.01401573,  0.01475363,  0.07300287,\n",
       "          0.02139688,  0.01332273,  0.01332273,  0.01760726,  0.01332273],\n",
       "        [ 0.02377679,  0.01371322,  0.01412826,  0.01339497,  0.01588219,\n",
       "          0.01339497,  0.01339497,  0.01339497,  0.02712917,  0.01339497,\n",
       "          0.01357653,  0.01339497,  0.01339497,  0.01339497,  0.01339497,\n",
       "          0.01339497,  0.01339497,  0.01339497,  0.01339497,  0.02278805,\n",
       "          0.01339497,  0.0169337 ,  0.01378056,  0.01542774,  0.02062591,\n",
       "          0.01339497,  0.02001718,  0.01388643,  0.01339497,  0.01339497,\n",
       "          0.01339497,  0.01339497,  0.01339497,  0.01339497,  0.01339497,\n",
       "          0.01339497,  0.01779158,  0.01341149,  0.01791604,  0.01339497,\n",
       "          0.01964055,  0.01339497,  0.01710587,  0.02112989,  0.02826163,\n",
       "          0.01511258,  0.0144245 ,  0.01339497,  0.01339497,  0.01987138,\n",
       "          0.02269066,  0.02244891,  0.01339497,  0.01402259,  0.0702346 ,\n",
       "          0.02180417,  0.01339497,  0.01339497,  0.01722373,  0.01339497],\n",
       "        [ 0.02385869,  0.01459548,  0.01413933,  0.01332876,  0.01550238,\n",
       "          0.01332876,  0.01332876,  0.01332876,  0.02604645,  0.01332876,\n",
       "          0.0136672 ,  0.01332876,  0.01332876,  0.01332876,  0.01332876,\n",
       "          0.01332876,  0.01332876,  0.01332876,  0.01332876,  0.02139394,\n",
       "          0.01332876,  0.01706166,  0.01388085,  0.01383179,  0.02128992,\n",
       "          0.01332876,  0.0199932 ,  0.01405898,  0.01332876,  0.01332876,\n",
       "          0.01332876,  0.01332876,  0.01332876,  0.01332876,  0.01332876,\n",
       "          0.01332876,  0.02037475,  0.01332876,  0.01754682,  0.01332876,\n",
       "          0.01928614,  0.01332876,  0.01759233,  0.01990603,  0.02842292,\n",
       "          0.01466323,  0.01453488,  0.01332876,  0.01332876,  0.02013861,\n",
       "          0.02236509,  0.02096861,  0.01435514,  0.01411064,  0.07345024,\n",
       "          0.02200333,  0.01332876,  0.01332876,  0.01776969,  0.01332876],\n",
       "        [ 0.02458378,  0.01437825,  0.01388464,  0.01330301,  0.01587879,\n",
       "          0.01330301,  0.01330301,  0.01330301,  0.02754211,  0.01330301,\n",
       "          0.01338585,  0.01330301,  0.01330301,  0.01330301,  0.01330301,\n",
       "          0.01330301,  0.01330301,  0.01330301,  0.01330301,  0.0206641 ,\n",
       "          0.01330301,  0.01764362,  0.0143298 ,  0.01374659,  0.01942374,\n",
       "          0.01330301,  0.01922995,  0.01414139,  0.01330301,  0.01330301,\n",
       "          0.01330301,  0.01330301,  0.01330301,  0.01330301,  0.01330301,\n",
       "          0.01330301,  0.01935542,  0.01340454,  0.01699263,  0.01330301,\n",
       "          0.01884797,  0.01330301,  0.01746855,  0.02042664,  0.02816636,\n",
       "          0.01409293,  0.01466947,  0.01330301,  0.01330301,  0.02077243,\n",
       "          0.02329451,  0.02125895,  0.01397803,  0.01551847,  0.07662489,\n",
       "          0.01997634,  0.01330301,  0.01330301,  0.01722891,  0.01330301],\n",
       "        [ 0.02340392,  0.01332291,  0.01406576,  0.01332291,  0.01669802,\n",
       "          0.01332291,  0.01332291,  0.01332291,  0.02717672,  0.01332291,\n",
       "          0.01332291,  0.01382972,  0.01332291,  0.01332291,  0.01332291,\n",
       "          0.01332291,  0.01332291,  0.01332291,  0.01332291,  0.023401  ,\n",
       "          0.01332291,  0.01715851,  0.0143248 ,  0.01620861,  0.01902568,\n",
       "          0.01332291,  0.01988334,  0.01396864,  0.01332291,  0.01332291,\n",
       "          0.01332291,  0.01332291,  0.01332291,  0.01332291,  0.01332291,\n",
       "          0.01332291,  0.01760377,  0.01332291,  0.01862258,  0.01332291,\n",
       "          0.01881231,  0.01332291,  0.016944  ,  0.02141143,  0.02902138,\n",
       "          0.01526627,  0.01439223,  0.01332291,  0.01366764,  0.02004592,\n",
       "          0.02202252,  0.02208939,  0.01332291,  0.01431744,  0.0705629 ,\n",
       "          0.02272166,  0.01332291,  0.01332291,  0.01702062,  0.01332291]], dtype=float32),\n",
       " array([[[[ 0.01911647,  0.01929476,  0.03522199, ...,  0.02558569,\n",
       "            0.01754827,  0.02256078]],\n",
       " \n",
       "         [[ 0.0335535 ,  0.02022287,  0.04086942, ...,  0.02508383,\n",
       "            0.04415853,  0.03429236]],\n",
       " \n",
       "         [[ 0.02978276,  0.04735582,  0.0202945 , ...,  0.04346332,\n",
       "            0.03546407,  0.0221865 ]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.04168317,  0.02600706,  0.03226594, ...,  0.02328081,\n",
       "            0.0230845 ,  0.02738037]],\n",
       " \n",
       "         [[ 0.02980973,  0.02047687,  0.0275577 , ...,  0.04099414,\n",
       "            0.02680169,  0.03334954]],\n",
       " \n",
       "         [[ 0.0383117 ,  0.0252256 ,  0.0217447 , ...,  0.0379557 ,\n",
       "            0.03786554,  0.02358583]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01712532,  0.01875427,  0.03456678, ...,  0.02455802,\n",
       "            0.01767727,  0.02391754]],\n",
       " \n",
       "         [[ 0.03745385,  0.03288907,  0.04465671, ...,  0.03397008,\n",
       "            0.03222939,  0.03905059]],\n",
       " \n",
       "         [[ 0.03858645,  0.04867538,  0.05327296, ...,  0.02188133,\n",
       "            0.02190017,  0.0413825 ]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.02554817,  0.04131864,  0.0490071 , ...,  0.02333772,\n",
       "            0.04423667,  0.02029352]],\n",
       " \n",
       "         [[ 0.02106347,  0.02684484,  0.02654025, ...,  0.04623314,\n",
       "            0.02036538,  0.03383074]],\n",
       " \n",
       "         [[ 0.03109648,  0.02031648,  0.039927  , ...,  0.04911293,\n",
       "            0.03732939,  0.04978004]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01741965,  0.0190256 ,  0.03403706, ...,  0.02474086,\n",
       "            0.01790937,  0.02399636]],\n",
       " \n",
       "         [[ 0.03640269,  0.02831564,  0.02472266, ...,  0.04856367,\n",
       "            0.02963126,  0.04556195]],\n",
       " \n",
       "         [[ 0.03067711,  0.03269618,  0.02171269, ...,  0.03315192,\n",
       "            0.02184282,  0.020069  ]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.04138722,  0.02980364,  0.04510676, ...,  0.0253694 ,\n",
       "            0.02319739,  0.03106784]],\n",
       " \n",
       "         [[ 0.03492498,  0.05085649,  0.02279059, ...,  0.03062097,\n",
       "            0.0228136 ,  0.03604027]],\n",
       " \n",
       "         [[ 0.03839929,  0.02886342,  0.03540086, ...,  0.02282987,\n",
       "            0.04937405,  0.01893891]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[ 0.01753498,  0.01850189,  0.03442186, ...,  0.02469386,\n",
       "            0.01885972,  0.02336101]],\n",
       " \n",
       "         [[ 0.02655589,  0.04666007,  0.04138996, ...,  0.01913301,\n",
       "            0.02991877,  0.03034268]],\n",
       " \n",
       "         [[ 0.04397459,  0.02894919,  0.04469819, ...,  0.02275343,\n",
       "            0.04787085,  0.02576218]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.03711201,  0.02040632,  0.02438495, ...,  0.03862733,\n",
       "            0.02985714,  0.02809168]],\n",
       " \n",
       "         [[ 0.02658708,  0.04371963,  0.04692237, ...,  0.0417832 ,\n",
       "            0.04553517,  0.02691306]],\n",
       " \n",
       "         [[ 0.02528121,  0.04456464,  0.03658399, ...,  0.02430785,\n",
       "            0.03945202,  0.03207894]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01704624,  0.01856297,  0.03505597, ...,  0.02480087,\n",
       "            0.01811443,  0.02381386]],\n",
       " \n",
       "         [[ 0.03455818,  0.03589336,  0.03600889, ...,  0.02841755,\n",
       "            0.02463787,  0.03297187]],\n",
       " \n",
       "         [[ 0.04471174,  0.04444999,  0.04454052, ...,  0.02486111,\n",
       "            0.05108203,  0.03507086]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.03196028,  0.02683525,  0.02327436, ...,  0.02180062,\n",
       "            0.0298761 ,  0.03487897]],\n",
       " \n",
       "         [[ 0.0355922 ,  0.02170624,  0.03531687, ...,  0.01996958,\n",
       "            0.0329192 ,  0.02492405]],\n",
       " \n",
       "         [[ 0.03722839,  0.02603448,  0.04900747, ...,  0.05164732,\n",
       "            0.03760954,  0.04378447]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01818964,  0.01920115,  0.03539883, ...,  0.02553178,\n",
       "            0.01856928,  0.02281209]],\n",
       " \n",
       "         [[ 0.02771738,  0.03710056,  0.04865773, ...,  0.02410417,\n",
       "            0.04311625,  0.03155037]],\n",
       " \n",
       "         [[ 0.03072684,  0.02811713,  0.03067135, ...,  0.02859725,\n",
       "            0.03533664,  0.02780751]],\n",
       " \n",
       "         ..., \n",
       "         [[ 0.0235248 ,  0.03663205,  0.01689565, ...,  0.03536059,\n",
       "            0.04411042,  0.02019797]],\n",
       " \n",
       "         [[ 0.04445039,  0.02839921,  0.03525678, ...,  0.05593394,\n",
       "            0.03352521,  0.0242398 ]],\n",
       " \n",
       "         [[ 0.02761201,  0.03893657,  0.02017799, ...,  0.02406553,\n",
       "            0.04426536,  0.04362451]]]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(fakeXf, fakeXi, fakeXs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from redbaron import RedBaron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/data/fs4/home/bradh/learningfunctions.py', 'r') as f:\n",
    "    testscript = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teststring = ''.join(testscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red = RedBaron(teststring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self.cost'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red.node_list[7].node_list[1].node_list[1].target.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class = 'ClassNode', def = 'DefNode', vars\n",
    "#funnames = []\n",
    "for fun in red.find_all('ClassNode'):\n",
    "    funnames.append(fun.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assignment'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#var names\n",
    "red.find_all('AssignmentNode')[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red = RedBaron(teststring)\n",
    "thirdKing = []\n",
    "\n",
    "def parseTree(red, thirdKing):\n",
    "    if not hasattr(red, 'node_list'):\n",
    "        return thirdKing\n",
    "    \n",
    "    for node in red.node_list:\n",
    "        #print(node.type)\n",
    "        if node.type == 'class':\n",
    "            classNode = [{'type': 'class', 'name': node.name}]\n",
    "            thirdKing.append(parseTree(node, classNode))\n",
    "            \n",
    "        elif node.type == 'def':\n",
    "            defNode = [{'type': 'def', 'name': node.name}]\n",
    "            thirdKing.append(parseTree(node, defNode))\n",
    "            \n",
    "        elif node.type == 'assignment':\n",
    "            varNode = [{'type': 'var', 'name': node.target.dumps()}]\n",
    "            thirdKing.append(parseTree(node, varNode))\n",
    "        \n",
    "    return thirdKing\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parsed = parseTree(red, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
